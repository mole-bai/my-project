{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5798f0f0-ef68-4630-866c-c6c6cd459fa7",
    "_uuid": "20b6dcf907bd223863cbbe5f4995c284282b05d9"
   },
   "source": [
    "# <a>Understanding and Implementing Neural Networks from scratch</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this kernel, I have explained the intution about neural networks and how to implement neural networks from scratch in python. \n",
    "\n",
    "## Contents  \n",
    "<br>\n",
    "\n",
    "**<a><i> 1. What are Neural Networks</i></a> **  \n",
    "\n",
    "**<a><i> 2. Implement a Neural Network - Binary classification</i></a>**   \n",
    "\n",
    "**<a><i> 3. Implement a Neural Network - Multiclass classification</i></a>**  \n",
    "\n",
    "**<a><i> 4. What are Deep Neural Networks</i></a> **  \n",
    "\n",
    "**<a><i> 5. Convolutional Neural Networks Implementation</i></a>**  \n",
    "\n",
    "![](https://www.pangeanic.com/wp-content/uploads/sites/2/2017/07/neural-network-graph-624x492.jpg)\n",
    "\n",
    "I would like to thank Andrew NG and deeplearning.ai course for their excellent material\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## <a>1. What are Neural Networks </a>\n",
    "\n",
    "Neural networks are a type of machine learning models which are designed to operate similar to biological neurons and human nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled dataset. They have following properties:\n",
    "\n",
    "1. The core architecture of a Neural Network model is comprised of a large number of simple processing nodes called Neurons which are interconnected and organized in different layers. \n",
    "\n",
    "2. An individual node in a layer is connected to several other nodes in the previous and the next layer. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n",
    "\n",
    "3. The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers. \n",
    "\n",
    "### Key concepts in a Neural Network \n",
    "\n",
    "#### A. Neuron:\n",
    "\n",
    "A Neuron is a single processing unit of a Neural Network which are connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a “weight” (W) which signifies the importance the input and adds a bias (b) term. \n",
    "\n",
    "#### B. Activation Functions \n",
    "\n",
    "The activation functions are used to apply non-linear transformation on input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH. \n",
    "\n",
    "#### C. Forward Propagation \n",
    "\n",
    "Neural Network model goes through the process called forward propagation in which it passes the computed activation outputs in the forward direction. \n",
    "\n",
    "Z = W*X + b   \n",
    "A = g(Z) \n",
    "\n",
    "- g is the activation function \n",
    "- A is the activation using the input \n",
    "- W is the weight associated with the input \n",
    "- B is the bias associated with the node \n",
    "\n",
    "#### D. Error Computation: \n",
    "\n",
    "The neural network learns by improving the values of weights and bias. The model computes the error in the predicted output in the final layer which is then used to make small adjustments the weights and bias. The adjustments are made such that the total error is minimized. Loss function measures the error in the final layer and cost function measures the total error of the network. \n",
    "\n",
    "Loss = Actual_Value - Predicted_Value   \n",
    "\n",
    "Cost = Summation (Loss)   \n",
    "\n",
    "#### E. Backward Propagation: \n",
    "\n",
    "Neural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. It uses the algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, derivative of weights, bias and subtracting them from the original values. \n",
    "\n",
    "<br>\n",
    "\n",
    "## <a> 2. Implement a Neural Network - Binary Classification</a>  \n",
    "\n",
    "Lets implement a basic neural network in python for binary classification which is used to classify if a given image is 0 or 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f"
   },
   "source": [
    "### 2.1 Dataset Preparation\n",
    "\n",
    "First step is to load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:/input/train.csv\")\n",
    "test = pd.read_csv(\"E:/input/test.csv\")\n",
    "\n",
    "# include only the rows having label = 0 or 1 (binary classification)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# target variable\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "\n",
    "# remove the label from X\n",
    "\n",
    "X = X.drop(['label'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "5       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "378fd937-d372-4eab-bcfd-46bf5817ed01",
    "_uuid": "37ea4b7a6bbedd53b00dec3cfefad523b44d68fb"
   },
   "source": [
    "### 2.2 Implementing a Activation Function \n",
    "\n",
    "We will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "19fc3ff1-f6ac-447d-98ea-4ff4d4bf4669",
    "_uuid": "13f0338a967a4cb87503821f044c09552d2e1640"
   },
   "outputs": [],
   "source": [
    "# implementing a sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    #########################需要填写，写出sigmoid的计算公式，并赋值给s，sigmoid=1/1+exp(x)\n",
    "    s = 1.0/ (1 + np.exp(-z))    \n",
    "    #########################需要填写，写出sigmoid的计算公式，并赋值给s，sigmoid=1/1+exp(x)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee1e1ebe-4298-40cd-9044-d8dc4327aa6c",
    "_uuid": "3c58c24a7feb9789a5bd84e8c9cc5068eebbee04"
   },
   "source": [
    "### 2.3 Define Neural Network Architecture\n",
    "\n",
    "Create a model with three layers - Input, Hidden, Output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a4c98dcc-6a34-4e5d-99d7-7e26ec12ce21",
    "_uuid": "deab1ad6439f304b669f70da3ca1f70e13f027f6"
   },
   "outputs": [],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    ##########################需要填写，计算每层的个数，输入层个数赋值给n_x,输出层赋值给n_y\n",
    "    n_x = X.shape[0] \n",
    "    # nodes in hidden layer\n",
    "    n_h = 10          \n",
    "    # nodes in output layer\n",
    "    n_y = Y.shape[0] \n",
    "     ##########################需要填写，计算每层的个数，输入层个数赋值给n_x,输出层赋值给n_y\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc75a514-66e5-4479-9613-7fdc4572271d",
    "_uuid": "c91f07b7c28808b74babea3d013fbfdd6ea3ba91"
   },
   "source": [
    "### 2.4 Define Neural Network Parameters \n",
    "\n",
    "Neural Network parameters are weights and bias which we need to initialze with zero values. The first layer only contains inputs so there are no weights and bias, but the hidden layer and the output layer have a weight and bias term. (W1, b1 and W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "cef24778-cd2c-4412-b017-7f4d9b34f34a",
    "_uuid": "c13e2d5cd4d04c05c878f7ccab1677b290847be4"
   },
   "outputs": [],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 # random initialization\n",
    "    b1 = np.zeros((n_h, 1)) # zero initialization\n",
    "    \n",
    "    ##############################需要填写，请为输出层的W2和b2赋值\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "    ##############################需要填写，请为输出层的W2和b2赋值\n",
    "    \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd1fd5e9-aae9-4996-8137-6f577147b90c",
    "_uuid": "9664d29beedd6c5a7c27b10201992a3c2f810c56"
   },
   "source": [
    "### 2.5 Implement Forward Propagation\n",
    "\n",
    "The hidden layer and output layer will compute the activations using sigmoid activation function and will pass it in the forward direction. While computing this activation, the input is multiplied with weight and added with bias before passing it to the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "2fd5cfe1-dbe2-4580-b541-1ed5cf139166",
    "_uuid": "ee7d22c45eeb9768b2722c36ecf9f73ff9ef2db9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    ################################需要填写，请计算Z2和A2\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    ################################需要填写，请计算Z2和A2\n",
    "    \n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da5fac38-7256-4fc3-bcfc-b2358b8bff31",
    "_uuid": "162413bf075111e6355ee9d5f16d0ca78a2f33a9"
   },
   "source": [
    "### 2.6 Compute the Network Error \n",
    "\n",
    "To compute the cost, one straight forward approach is to compute the absolute error among prediction and actual value. But a better loss function is the log loss function which is defines as : \n",
    "\n",
    "  -Summ ( Log (Pred) * Actual + Log (1 - Pred ) * Actual ) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "155f019d-3e33-4cdd-9722-4d0a7121276f",
    "_uuid": "08d7b796a3dbdcb1530053238037803ac76362c3"
   },
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):                   #计算误差函数，可以选择不同的误差函数\n",
    "    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n",
    "    cost = -np.sum(logprobs) / Actual.shape[1] \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c4ef52b-55ac-4ef2-a5fd-ee164db8dcdc",
    "_uuid": "58d61c5257c8f568d439a605fc311f483f09f1e2"
   },
   "source": [
    "### 2.7 Implement Backward Propagation\n",
    "\n",
    "In backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8796bbb2-8fbf-4740-9687-c3ec2c38a27f",
    "_uuid": "195d2b62d4655b7ecff22e4bc70be15256aadf1b"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):                ##########反向传播算法，计算导数############\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    dZ2 = activations['A2'] - Y # compute the error derivative \n",
    "    dW2 = np.dot(dZ2, activations['A1'].T) / m # compute the weight derivative \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m # compute the bias derivative\n",
    "    \n",
    "    # hidden layer\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activations['A1'], 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True)/m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):           ##########反向传播算法，更新参数############\n",
    "    # alpha is the model's learning rate \n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    \n",
    "    ####################需要填写，请计算W2和b2更新之后的值\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    ####################需要填写，请计算W2和b2更新之后的值\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "935845e8-c35f-43ae-8f28-65eee43fc428",
    "_uuid": "1724d52016b387a2f974e6c6fc27c2a17a96166b"
   },
   "source": [
    "### 2.8 Compile and Train the Model\n",
    "\n",
    "Create a function which compiles all the key functions and creates a neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "84c99459-755e-4795-a52f-bb8cf59299a4",
    "_uuid": "011debd87ff12ce75d29dac9383d74db9c64a35a"
   },
   "outputs": [],
   "source": [
    "def neural_network(X, Y, n_h, num_iterations=100):                      ############编译网络\n",
    "    n_x = network_architecture(X, Y)[0]\n",
    "    n_y = network_architecture(X, Y)[2]\n",
    "    \n",
    "    params = define_network_parameters(n_x, n_h, n_y)                  ############初始化参数  \n",
    "    for i in range(0, num_iterations):\n",
    "        results = forward_propagation(X, params)                       ############前向传播 \n",
    "        error = compute_error(results['A2'], Y)\n",
    "        derivatives = backward_propagation(params, results, X, Y)      ############反向传播\n",
    "        params = update_parameters(params, derivatives)                ############更新参数\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "fb2fa7af-e787-4ca1-9079-f6dfb6723390",
    "_uuid": "2d983cbb3c77270b2ee66d9148315cb24888f292"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "y = Y.values.reshape(1, Y.size)\n",
    "x = X.T.as_matrix()\n",
    "model = neural_network(x, y, n_h = 10, num_iterations = 10)            ############生成一个新的网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6dccfd5a-8850-41a0-b406-47460f5bd2ed",
    "_uuid": "dc33c60645cdaba20c5a420ab6bc7e5933b6e88b"
   },
   "source": [
    "### 2.9 Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "59eed767-201d-4138-a77e-2ce8eaad83df",
    "_uuid": "df27eb0a992e1d7646c2ef5f5c7ca928bca6155e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52351746 0.10311941 0.95776857 ... 0.95776857 0.10311941 0.95776857]\n",
      "Accuracy: 96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, x)\n",
    "print ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))/float(y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
